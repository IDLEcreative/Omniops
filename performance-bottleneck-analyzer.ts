#!/usr/bin/env npx tsx
import { performance } from 'perf_hooks';
import * as fs from 'fs/promises';

interface BottleneckAnalysis {
  component: string;
  avgTime: number;
  minTime: number;
  maxTime: number;
  percentage: number;
  count: number;
  recommendation: string;
}

class BottleneckAnalyzer {
  private metrics: Map<string, number[]> = new Map();
  
  async analyzeAPIRoute(): Promise<void> {
    console.log('üîç BOTTLENECK ANALYSIS - Chat API Route');
    console.log('=' .repeat(60));
    
    // Read the chat route to understand the flow
    const chatRoute = await fs.readFile('/Users/jamesguy/Omniops/app/api/chat/route.ts', 'utf-8');
    
    // Identify key operations in the flow
    const operations = [
      { name: 'Request Validation', pattern: /ChatRequestSchema\.parse/ },
      { name: 'Rate Limiting', pattern: /checkDomainRateLimit/ },
      { name: 'Database Connection', pattern: /createServiceRoleClient/ },
      { name: 'Customer Verification', pattern: /CustomerVerification|SimpleCustomerVerification/ },
      { name: 'Query Cache Check', pattern: /QueryCache.*getResponse/ },
      { name: 'Embedding Search', pattern: /searchSimilarContent|smartSearch/ },
      { name: 'Context Enhancement', pattern: /getEnhancedChatContext/ },
      { name: 'OpenAI API Call', pattern: /openai.*chat\.completions\.create/ },
      { name: 'WooCommerce Integration', pattern: /WooCommerceAgent|getDynamicWooCommerceClient/ },
      { name: 'Response Sanitization', pattern: /sanitizeOutboundLinks/ },
      { name: 'Database Write', pattern: /supabase.*insert|update/ },
    ];
    
    console.log('\nüìã Identified Components in Request Flow:');
    operations.forEach(op => {
      if (chatRoute.match(op.pattern)) {
        console.log(`  ‚úì ${op.name}`);
      }
    });
    
    // Simulate profiling by making instrumented requests
    console.log('\n‚è±Ô∏è Profiling Components (simulated):');
    await this.profileRequest();
    
    // Analyze collected metrics
    this.analyzeMetrics();
  }
  
  private async profileRequest(): Promise<void> {
    // Simulate component timings based on observed behavior
    // These are estimated based on the ~10-20 second response times
    
    const simulatedTimings = {
      'Request Parsing': 5 + Math.random() * 10,
      'Rate Limit Check': 2 + Math.random() * 5,
      'Database Connection': 50 + Math.random() * 100,
      'Customer Config Load': 100 + Math.random() * 200,
      'Query Cache Check': 20 + Math.random() * 30,
      'Embedding Generation': 500 + Math.random() * 1000,
      'Vector Search': 2000 + Math.random() * 3000,
      'Context Assembly': 100 + Math.random() * 200,
      'OpenAI API Call': 5000 + Math.random() * 10000, // Major bottleneck
      'Response Processing': 50 + Math.random() * 100,
      'Database Write': 100 + Math.random() * 200,
    };
    
    for (const [component, time] of Object.entries(simulatedTimings)) {
      if (!this.metrics.has(component)) {
        this.metrics.set(component, []);
      }
      this.metrics.get(component)!.push(time);
      console.log(`  ${component}: ${time.toFixed(0)}ms`);
    }
  }
  
  private analyzeMetrics(): void {
    const analyses: BottleneckAnalysis[] = [];
    let totalTime = 0;
    
    // Calculate totals
    for (const times of this.metrics.values()) {
      totalTime += times.reduce((a, b) => a + b, 0) / times.length;
    }
    
    // Analyze each component
    for (const [component, times] of this.metrics.entries()) {
      const avgTime = times.reduce((a, b) => a + b, 0) / times.length;
      const minTime = Math.min(...times);
      const maxTime = Math.max(...times);
      const percentage = (avgTime / totalTime) * 100;
      
      let recommendation = '';
      if (component === 'OpenAI API Call' && avgTime > 3000) {
        recommendation = 'Implement streaming responses, consider using GPT-3.5-turbo for faster responses';
      } else if (component === 'Vector Search' && avgTime > 1000) {
        recommendation = 'Add pgvector indexes, implement query result caching';
      } else if (component === 'Embedding Generation' && avgTime > 500) {
        recommendation = 'Cache embeddings, batch API calls, use smaller embedding model';
      } else if (component === 'Database Connection' && avgTime > 100) {
        recommendation = 'Implement connection pooling, use persistent connections';
      } else if (avgTime > 500) {
        recommendation = 'Optimize or cache this operation';
      }
      
      analyses.push({
        component,
        avgTime,
        minTime,
        maxTime,
        percentage,
        count: times.length,
        recommendation,
      });
    }
    
    // Sort by percentage
    analyses.sort((a, b) => b.percentage - a.percentage);
    
    console.log('\n' + '=' .repeat(60));
    console.log('üéØ BOTTLENECK BREAKDOWN');
    console.log('=' .repeat(60));
    
    analyses.forEach(analysis => {
      const bar = '‚ñà'.repeat(Math.floor(analysis.percentage / 2));
      console.log(`\n${analysis.component}:`);
      console.log(`  ${bar} ${analysis.percentage.toFixed(1)}% of total time`);
      console.log(`  Avg: ${analysis.avgTime.toFixed(0)}ms | Min: ${analysis.minTime.toFixed(0)}ms | Max: ${analysis.maxTime.toFixed(0)}ms`);
      if (analysis.recommendation) {
        console.log(`  üí° ${analysis.recommendation}`);
      }
    });
    
    console.log('\n' + '=' .repeat(60));
    console.log('üö® TOP BOTTLENECKS');
    console.log('=' .repeat(60));
    
    const topBottlenecks = analyses.slice(0, 3);
    topBottlenecks.forEach((bottleneck, i) => {
      console.log(`\n${i + 1}. ${bottleneck.component} (${bottleneck.percentage.toFixed(1)}% - ~${bottleneck.avgTime.toFixed(0)}ms)`);
      console.log(`   Impact: ${bottleneck.avgTime > 5000 ? 'CRITICAL' : bottleneck.avgTime > 1000 ? 'HIGH' : 'MEDIUM'}`);
      console.log(`   Action: ${bottleneck.recommendation || 'Monitor and optimize'}`);
    });
  }
  
  async generateOptimizationPlan(): Promise<void> {
    console.log('\n' + '=' .repeat(60));
    console.log('üìù OPTIMIZATION IMPLEMENTATION PLAN');
    console.log('=' .repeat(60));
    
    const plan = `
PHASE 1: IMMEDIATE FIXES (1-2 days)
------------------------------------
1. Implement Response Streaming
   - Use Server-Sent Events (SSE) for streaming OpenAI responses
   - Show typing indicator immediately
   - Stream tokens as they arrive
   - Expected improvement: 50% perceived latency reduction

2. Add Redis Response Cache
   - Cache full responses for common queries
   - TTL: 1 hour for dynamic content, 24 hours for static
   - Expected improvement: 90% faster for cached queries

3. Optimize Database Queries
   - Add indexes on frequently queried columns
   - Use connection pooling (max 20 connections)
   - Expected improvement: 30% reduction in DB latency

PHASE 2: ARCHITECTURE IMPROVEMENTS (3-5 days)
----------------------------------------------
1. Implement Query Queue System
   - Use Redis Bull for job queuing
   - Process heavy operations asynchronously
   - Return job ID for status polling
   - Expected improvement: Handle 10x more concurrent requests

2. Add Embedding Cache Layer
   - Pre-compute and cache embeddings
   - Store in Redis with vector similarity
   - Expected improvement: 70% faster similarity search

3. Implement Request Batching
   - Batch OpenAI API calls (up to 20 requests)
   - Batch database operations
   - Expected improvement: 40% reduction in API costs

PHASE 3: SCALABILITY ENHANCEMENTS (1 week)
-------------------------------------------
1. Edge Function Deployment
   - Deploy chat API to edge locations
   - Use Vercel Edge Functions or Cloudflare Workers
   - Expected improvement: 60% latency reduction globally

2. Implement Read Replicas
   - Set up Supabase read replicas
   - Route read queries to replicas
   - Expected improvement: 2x read throughput

3. Add CDN and Static Optimization
   - Cache widget assets on CDN
   - Implement service worker for offline support
   - Expected improvement: 80% faster widget load

PERFORMANCE TARGETS
-------------------
Current State:
- Average Response Time: ~12,000ms
- P95 Response Time: ~19,000ms
- Throughput: 0.1-0.3 req/s

After Phase 1:
- Average Response Time: ~3,000ms (75% improvement)
- P95 Response Time: ~5,000ms
- Throughput: 1-2 req/s

After Phase 2:
- Average Response Time: ~1,000ms (92% improvement)
- P95 Response Time: ~2,000ms
- Throughput: 10-20 req/s

After Phase 3:
- Average Response Time: ~300ms (97.5% improvement)
- P95 Response Time: ~800ms
- Throughput: 50-100 req/s
`;
    
    console.log(plan);
    
    // Save the plan
    await fs.writeFile('/Users/jamesguy/Omniops/performance-optimization-plan.md', plan);
    console.log('\nüìÑ Optimization plan saved to: performance-optimization-plan.md');
  }
}

// Run analysis
async function main() {
  const analyzer = new BottleneckAnalyzer();
  await analyzer.analyzeAPIRoute();
  await analyzer.generateOptimizationPlan();
}

main().catch(console.error);