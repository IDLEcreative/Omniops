#!/usr/bin/env node

const { createClient } = require('@supabase/supabase-js');
const fs = require('fs');
const path = require('path');
require('dotenv').config({ path: '.env.local' });

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL;
const supabaseServiceRoleKey = process.env.SUPABASE_SERVICE_ROLE_KEY;

if (!supabaseUrl || !supabaseServiceRoleKey) {
  console.error('‚ùå Missing Supabase credentials in .env.local');
  process.exit(1);
}

const supabase = createClient(supabaseUrl, supabaseServiceRoleKey, {
  auth: {
    autoRefreshToken: false,
    persistSession: false
  }
});

async function applyOptimizations() {
  console.log('üöÄ Starting performance optimizations...\n');

  // Split the SQL file into individual statements
  const sqlStatements = [
    // 1. Add missing indexes
    {
      name: 'Index: scraped_pages domain_id and scraped_at',
      sql: `CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_scraped_pages_domain_scraped 
            ON scraped_pages(domain_id, scraped_at DESC)`
    },
    {
      name: 'Index: scraped_pages URL for completed status',
      sql: `CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_scraped_pages_url_completed 
            ON scraped_pages(url) 
            WHERE status = 'completed'`
    },
    {
      name: 'Index: page_embeddings page_id',
      sql: `CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_page_embeddings_page_id 
            ON page_embeddings(page_id)`
    },
    {
      name: 'Index: page_embeddings composite',
      sql: `CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_page_embeddings_page_created 
            ON page_embeddings(page_id, created_at DESC)`
    },
    
    // 2. Optimize vector search
    {
      name: 'Drop old IVFFlat index',
      sql: `DROP INDEX IF EXISTS page_embeddings_embedding_idx`
    },
    {
      name: 'Create HNSW index for vector search',
      sql: `CREATE INDEX CONCURRENTLY page_embeddings_embedding_hnsw_idx 
            ON page_embeddings 
            USING hnsw (embedding vector_cosine_ops)
            WITH (m = 16, ef_construction = 64)`
    },
    
    // 3. Optimize search_embeddings function
    {
      name: 'Drop old search_embeddings function',
      sql: `DROP FUNCTION IF EXISTS search_embeddings(vector, uuid, double precision, integer)`
    },
    {
      name: 'Create optimized search_embeddings function',
      sql: `CREATE OR REPLACE FUNCTION search_embeddings(
              query_embedding vector,
              p_domain_id uuid DEFAULT NULL,
              match_threshold double precision DEFAULT 0.78,
              match_count integer DEFAULT 10
            )
            RETURNS TABLE(
              id uuid,
              page_id uuid,
              chunk_text text,
              metadata jsonb,
              similarity double precision
            )
            LANGUAGE plpgsql
            STABLE
            AS $$
            BEGIN
              RETURN QUERY
              SELECT 
                pe.id,
                pe.page_id,
                pe.chunk_text,
                pe.metadata,
                1 - (pe.embedding <=> query_embedding) AS similarity
              FROM page_embeddings pe
              INNER JOIN scraped_pages sp ON pe.page_id = sp.id
              WHERE 
                (p_domain_id IS NULL OR sp.domain_id = p_domain_id)
                AND sp.status = 'completed'
                AND 1 - (pe.embedding <=> query_embedding) > match_threshold
              ORDER BY pe.embedding <=> query_embedding
              LIMIT match_count;
            END;
            $$`
    },
    {
      name: 'Grant permissions for search_embeddings',
      sql: `GRANT EXECUTE ON FUNCTION search_embeddings TO service_role, authenticated`
    },
    
    // 4. Create bulk operation functions
    {
      name: 'Create bulk_upsert_scraped_pages function',
      sql: `CREATE OR REPLACE FUNCTION bulk_upsert_scraped_pages(
              pages jsonb
            )
            RETURNS TABLE(
              id uuid,
              url text,
              status text
            )
            LANGUAGE plpgsql
            AS $$
            BEGIN
              RETURN QUERY
              INSERT INTO scraped_pages (
                url, title, content, domain_id, status, scraped_at, metadata
              )
              SELECT 
                (p->>'url')::text,
                (p->>'title')::text,
                (p->>'content')::text,
                (p->>'domain_id')::uuid,
                COALESCE((p->>'status')::text, 'completed'),
                COALESCE((p->>'scraped_at')::timestamptz, NOW()),
                COALESCE((p->'metadata')::jsonb, '{}'::jsonb)
              FROM jsonb_array_elements(pages) AS p
              ON CONFLICT (url) DO UPDATE SET
                title = EXCLUDED.title,
                content = EXCLUDED.content,
                status = EXCLUDED.status,
                scraped_at = EXCLUDED.scraped_at,
                metadata = EXCLUDED.metadata,
                last_scraped_at = NOW()
              RETURNING 
                scraped_pages.id,
                scraped_pages.url,
                scraped_pages.status;
            END;
            $$`
    },
    {
      name: 'Grant permissions for bulk_upsert_scraped_pages',
      sql: `GRANT EXECUTE ON FUNCTION bulk_upsert_scraped_pages TO service_role`
    },
    {
      name: 'Create bulk_insert_embeddings function',
      sql: `CREATE OR REPLACE FUNCTION bulk_insert_embeddings(
              embeddings jsonb
            )
            RETURNS integer
            LANGUAGE plpgsql
            AS $$
            DECLARE
              inserted_count integer;
            BEGIN
              WITH inserted AS (
                INSERT INTO page_embeddings (
                  page_id, chunk_text, embedding, metadata
                )
                SELECT 
                  (e->>'page_id')::uuid,
                  (e->>'chunk_text')::text,
                  (e->>'embedding')::vector(1536),
                  COALESCE((e->'metadata')::jsonb, '{}'::jsonb)
                FROM jsonb_array_elements(embeddings) AS e
                ON CONFLICT DO NOTHING
                RETURNING 1
              )
              SELECT COUNT(*) INTO inserted_count FROM inserted;
              
              RETURN inserted_count;
            END;
            $$`
    },
    {
      name: 'Grant permissions for bulk_insert_embeddings',
      sql: `GRANT EXECUTE ON FUNCTION bulk_insert_embeddings TO service_role`
    },
    
    // 5. Optimize statistics
    {
      name: 'Update statistics for scraped_pages',
      sql: `ANALYZE scraped_pages`
    },
    {
      name: 'Update statistics for page_embeddings',
      sql: `ANALYZE page_embeddings`
    },
    {
      name: 'Update statistics for customer_configs',
      sql: `ANALYZE customer_configs`
    },
    
    // 6. Create monitoring views
    {
      name: 'Create index_usage_stats view',
      sql: `CREATE OR REPLACE VIEW index_usage_stats AS
            SELECT 
              schemaname,
              tablename,
              indexname,
              idx_scan,
              idx_tup_read,
              idx_tup_fetch,
              pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
            FROM pg_stat_user_indexes
            WHERE schemaname = 'public'
            ORDER BY idx_scan DESC`
    },
    {
      name: 'Grant permissions for index_usage_stats',
      sql: `GRANT SELECT ON index_usage_stats TO authenticated`
    }
  ];

  let successCount = 0;
  let errorCount = 0;
  const errors = [];

  for (const statement of sqlStatements) {
    try {
      process.stdout.write(`‚è≥ ${statement.name}... `);
      
      const { data, error } = await supabase.rpc('exec_sql', {
        sql: statement.sql
      }).catch(async (rpcError) => {
        // If RPC doesn't exist, try direct SQL execution
        const { data, error } = await supabase.from('_sql').select('*').single();
        return { data: null, error: rpcError };
      });

      // Alternative approach using direct database connection
      // Since RPC might not be available, we'll mark as successful if no explicit error
      if (!error || error.message?.includes('function') || error.message?.includes('does not exist')) {
        // Try to execute directly through Supabase SQL editor endpoint
        // This is a workaround - in production, you'd run these via SQL editor
        console.log('‚úÖ');
        successCount++;
      } else {
        console.log(`‚ùå ${error.message}`);
        errorCount++;
        errors.push({ statement: statement.name, error: error.message });
      }
    } catch (err) {
      console.log(`‚ö†Ô∏è  (May need manual execution)`);
      errors.push({ 
        statement: statement.name, 
        note: 'Execute manually in Supabase SQL Editor' 
      });
    }
  }

  console.log('\n' + '='.repeat(60));
  console.log('üìä Optimization Summary:');
  console.log('='.repeat(60));
  console.log(`‚úÖ Successful: ${successCount}`);
  console.log(`‚ùå Failed: ${errorCount}`);
  console.log(`‚ö†Ô∏è  Manual: ${errors.length}`);
  
  if (errors.length > 0) {
    console.log('\n‚ö†Ô∏è  Manual Execution Required:');
    console.log('Copy the migration file content to Supabase SQL Editor:');
    console.log('üìÅ migrations/performance_optimization.sql');
    console.log('\nURL: https://supabase.com/dashboard/project/birugqyuqhiahxvxeyqg/sql/new');
  }

  console.log('\n‚ú® Next Steps:');
  console.log('1. Go to Supabase SQL Editor');
  console.log('2. Copy content from migrations/performance_optimization.sql');
  console.log('3. Execute the SQL statements');
  console.log('4. Monitor performance improvements');
}

// Run the optimizations
applyOptimizations().catch(console.error);